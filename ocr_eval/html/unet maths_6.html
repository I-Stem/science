<html><head><style>
        table, th, td {
          border: 1px solid black;
          border-collapse: collapse;
        }
        </style></head><body><h6>Page 1</h6><span idx = 1><h6>Column 1</h6><p id=0>W. Ohyama et al.: Detecting MEs in Scientific Document Images Using a U-Net Trained on a Diverse Dataset</p><p id=1>where X and Y are the binary image of the network output and that of the ground truth. X n Y denotes the overlap between X and Y, and |X | is the L1-norm of image X.</p><p id=2>The proposed method does not use any information from either mathematical grammar or the character recognition results. The image conversion module in the proposed method is requested to obtain information that is crucial to determine the components that should remain as MEs only from the appearance of documents in the surrounding image area. A limited size of small regions may cause difficulty regarding making a decision even for humans in this scenario.</p><h1>C. POSTPROCESSING</h1><p id=3>Through the image conversion process, we obtain sub-block images that contain CCs that correspond to MEs. In the postprocessing stage, we reconstruct the page image and extract CCs that correspond to mathematical symbols and characters.</p><p id=4>To reconstruct the entire page image, each sub-block image is rearranged in the equivalent position and the maximum pixel value among the overlapping pixels is assigned to the corresponding pixel in the page image.</p><p id=5>Additionally, pixel-wise multiplication between the resized reconstructed image and the original image is performed to eliminate dilated pixels caused by the morphology operation in preprocessing, and artifacts and noise injected during the image conversion process.</p><h1>IV. PERFORMANCE EVALUATION</h1><h1>A. DATASETS</h1><p id=6>For a quantitative evaluation of the performance of mathematical OCR, a number of datasets have been proposed in the literature. InftyCDB datasets [39], [40] are large collections of mathematical symbols and notation from actual mathematical documents. UW databases [41] contain 100 typeset MEs from 25 document pages. However, these datasets are not applicable for evaluating ME detection performance because the content in the dataset is rearranged not to keep the original articles because of copyright reasons.</p><p id=7>We collected two large datasets to train U-Net and evaluate the performance of the proposed ME detection method. The datasets, called GTDB-1 and GTDB-2, consist of document page images collected from scientific journals and textbooks. The GTDB-1 dataset, which was used to train the U-Net model, contains 31 English articles on mathematics. The GTDB-2 dataset, which was used for quantitative and qualitative evaluations of the performance of the proposed method, contains 16 articles. Diverse font faces and mathematical notation styles are included in these articles. A list of the articles in both datasets is provided in the appendix.</p><p id=8>The statistics of each dataset are shown in Table 1. The article pages were originally scanned at 600 dpi. The ground VOLUME 7 , 2019</p></span><span idx = 2><h6>Column 2</h6><p id=0>IEEE Access*</p><p id=1>TABLE 1. Statistics of the datasets: Two datasets collected from scientific journals and textbooks.</p><table><tr><td></td><td>GTDB- 1 </td><td>GTDB-2 </td></tr><tr><td># articles </td><td>31 </td><td>16 </td></tr><tr><td># pages </td><td>544 </td><td>343 </td></tr><tr><td># math symbols </td><td>162,406 </td><td>115,433 </td></tr><tr><td># of ordinary text characters </td><td>646,714 </td><td>507,412 </td></tr></table></br></br><p id=2>truth annotations for each math symbol and ordinary character were attached manually. 2</p><h1>B. EVALUATION EXPERIMENTS</h1><p id=3>To train the U-Net model, we extracted 1,000 pairs of sub- blocks from each document page and the corresponding ground truth image from the GTDB-1 dataset. The locations of sub-blocks on each page image were determined randomly. The dataset consisted of 544 images, so the total number of sub-block images for training was 544,000.</p><p id=4>We mainly used mathematical symbol (character) recall Rs, precision Ps and F-measure Fs as the performance measures. Each measure is defined as follows: nTP R$ =</p><ol type=1 start= 2><ol type=1 start= 2><li>  NTP + nEN nTP Ps =</li><li>  nTP + nFP 2 PsRs Fs =</li><li>  Ps + Rs</li></ol><li> </li></ol><p id=5>where nTP, nFP and nEN are the numbers of correctly detected mathematical symbols, falsely detected symbols or ordinary text, and undetected mathematical symbols, respectively. Pixel-level majority voting is used for the symbol-level evaluation. If the majority of pixels in a candidate symbol were detected as a mathematical symbol, the candidate symbol is determined as a mathematical symbol. We also used ME-based recall (Re), precision (Pe) and</p><p id=6>F-measure (Fe) as supplemental performance measures. Their definitions are similar to (2)-(4), but the numbers in the equations are counted for regions.</p><p id=7>To determine MEs over the detected mathematical symbols, mathematical layout analysis is requested to obtain the spatial relationship between the symbols. We do not intend to implement layout analysis in the present study. Therefore, note that the evaluations using ME-based measures are based on the assumption that the candidates of mathematical regions are obtained using some layout analysis method. In this study, we extracted candidate regions using the ground truth so that the candidate regions contain in-line and displayed MEs, and</p><p id=8>2 The ground truth annotation has been released to the public to benchmark OCR performance for scientific documents. Although we could not include the original document images of articles for copyright reasons, we provide hyperlinks on our website to the web pages of the original documents, where the readers can obtain the document images: https://github.com/ uchidalab/GTDB-Dataset/tree/master 144035</p></span><body></html>