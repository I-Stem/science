OCR:	wo  	null	models	use	selective	search	to	find	out	the	region	pro-	posals,	while	selective	search	is	a	slow	and	
GT :	null	two 	models	use	selective	search	to	find	out	the	region	pro-	posals,	while	selective	search	is	a	slow	and	

OCR:	time-consuming	process	affecting	the	performance	of	the	network.	Distinct	from	two	previous	methods,	the	Faster	R-CNN	method	in-	troduces	a	
GT :	time-consuming	process	affecting	the	performance	of	the	network.	Distinct	from	two	previous	methods,	the	Faster	R-CNN	method	in-	troduces	a	

OCR:	Region	Proposal	Network	(RPN)	that	shares	full-	mage	null 	convolutional	features	with	the	detection	network,	thus	enabling	nearly	cost-free	region	
GT :	Region	Proposal	Network	(RPN)	that	shares	full-	null	image	convolutional	features	with	the	detection	network,	thus	enabling	nearly	cost-free	region	

OCR:	proposals.	Furthermore,	the	model	merges	RPN	and	Fast	R-CNN	into	a	single	network	by	sharing	their	convolutional	features	so	that	
GT :	proposals.	Furthermore,	the	model	merges	RPN	and	Fast	R-CNN	into	a	single	network	by	sharing	their	convolutional	features	so	that	

OCR:	the	network	can	be	trained	in	an	end-to-end	way.	The	overall	architecture	of	Faster	R-CNN	in	shown	in	Figure	5.	
GT :	the	network	can	be	trained	in	an	end-to-end	way.	The	overall	architecture	of	Faster	R-CNN	in	shown	in	Figure	5.	

OCR:	4.2	Table	Structure	Recognition	We	leverage	the	image-to-text	model	as	the	baseline.	The	mage-to-text	null         	model	has	been	widely	used	
GT :	4.2	Table	Structure	Recognition	We	leverage	the	image-to-text	model	as	the	baseline.	The	null        	image-to-text	model	has	been	widely	used	

OCR:	in	image	caption-	ing,	video	description,	and	many	other	applications.	A	typical	mage-to-text	null         	model	includes	an	encoder	for	the	
GT :	in	image	caption-	ing,	video	description,	and	many	other	applications.	A	typical	null        	image-to-text	model	includes	an	encoder	for	the	

OCR:	image	input	and	a	decoder	for	the	text	output.	In	this	work,	we	use	the	image-to-markup	model	[Deng	rain	null	
GT :	image	input	and	a	decoder	for	the	text	output.	In	this	work,	we	use	the	image-to-markup	model	[Deng	null	et  	

OCR:	null	null 	null	null	null    	null	null 	models	on	the	TableBank	dataset.	The	overall	architec	null     	ture	of	the	image-to-text	
GT :	al.,	2016]	as  	the 	baseline	to  	train	models	on	the	TableBank	dataset.	The	overall	null    	architec-	ture	of	the	image-to-text	

OCR:	model	is	shown	in	Figure	6.	encoder	decoder	i   	Figure	6:	Image-to-Text	model	for	table	structure	recognition	5	Experiment	5.1	
GT :	model	is	shown	in	Figure	6.	null   	null   	null	Figure	6:	Image-to-Text	model	for	table	structure	recognition	5	Experiment	5.1	

OCR:	Data	and	Metrics	The	statistics	of	TableBank	is	shown	in	Table	1.	To	evalu-	ate	table	detection,	we	sample	2,000	
GT :	Data	and	Metrics	The	statistics	of	TableBank	is	shown	in	Table	1.	To	evalu-	ate	table	detection,	we	sample	2,000	

OCR:	document	images	from	Word	and	Latex	documents	respectively,	where	1,000	images	or  	null	validation	and	1,000	images	for	testing.	Each	
GT :	document	images	from	Word	and	Latex	documents	respectively,	where	1,000	images	null	for 	validation	and	1,000	images	for	testing.	Each	

OCR:	sampled	im-	age	contains	at	least	one	table.	Meanwhile,	we	also	evaluate	our	model	on	the	ICDAR	2013	dataset	to	
GT :	sampled	im-	age	contains	at	least	one	table.	Meanwhile,	we	also	evaluate	our	model	on	the	ICDAR	2013	dataset	to	

OCR:	verify	the	effective-	ness	of	TableBank.	To	evaluate	table	structure	recognition,	we	sample	500	tables	each	for	validation	and	testing	
GT :	verify	the	effective-	ness	of	TableBank.	To	evaluate	table	structure	recognition,	we	sample	500	tables	each	for	validation	and	testing	

OCR:	from	Word	documents	and	Latex	documents	respectively.	The	en-	tire	training	and	testing	data	will	be	made	available	to	the	
GT :	from	Word	documents	and	Latex	documents	respectively.	The	en-	tire	training	and	testing	data	will	be	made	available	to	the	

OCR:	pub-	ic  	null	soon.	For	table	detection,	we	calculate	the	precision,	re-	1   	null	null	null	in	the	same	way	
GT :	pub-	null	lic 	soon.	For	table	detection,	we	calculate	the	precision,	re-	null	call	and 	F1  	in	the	same	way	

OCR:	as	in	[Gilani	et	al.,	20171,	null  	where	the	metrics	for	all	documents	are	computed	by	summing	up	the	area	
GT :	as	in	[Gilani	et	al.,	null  	2017],	where	the	metrics	for	all	documents	are	computed	by	summing	up	the	area	

OCR:	of	overlap,	prediction	and	ground	truth.	For	table	structure	recognition,	we	use	the	4-gram	BLEU	score	as	the	evaluation	metric	
GT :	of	overlap,	prediction	and	ground	truth.	For	table	structure	recognition,	we	use	the	4-gram	BLEU	score	as	the	evaluation	metric	

OCR:	with	a	single	reference.	t   	Table	1:	Statistics	of	TableBank	5.2	Settings	For	table	detection,	we	use	the	open	source	
GT :	with	a	single	reference.	null	Table	1:	Statistics	of	TableBank	5.2	Settings	For	table	detection,	we	use	the	open	source	

OCR:	framework	Detec-	tron	[Girshick	et	al.,	2018]	to	train	models	on	the	TableBank.	Detectron	is	a	high-quality	and	high-performance	codebase	
GT :	framework	Detec-	tron	[Girshick	et	al.,	2018]	to	train	models	on	the	TableBank.	Detectron	is	a	high-quality	and	high-performance	codebase	

OCR:	for	object	detection	research,	which	supports	many	state-of-	the-art	algorithms.	In	this	task,	we	use	the	Faster	R-CNN	al-	gorithm	
GT :	for	object	detection	research,	which	supports	many	state-of-	the-art	algorithms.	In	this	task,	we	use	the	Faster	R-CNN	al-	gorithm	

OCR:	with	the	ResNext	null   	[Xie	et	al.,	2016)	null 	as	the	backbone	network	architecture,	where	the	parameters	are	pre-trained	on	
GT :	with	the	null   	ResNeXt	[Xie	et	al.,	null 	2016]	as	the	backbone	network	architecture,	where	the	parameters	are	pre-trained	on	

OCR:	the	ImageNet	dataset.	All	baselines	are	trained	using	4xP100	IA  	null  	null  	GPUs	using	data	parallel	sync	SGD	with	a	
GT :	the	ImageNet	dataset.	All	baselines	are	trained	using	null  	null	4×P100	NVIDIA	GPUs	using	data	parallel	sync	SGD	with	a	

OCR:	mini-	batch	size	of	16	images.	For	other	parameters,	we	use	the	default	values	in	Detectron.	During	testing,	the	confidence	
GT :	mini-	batch	size	of	16	images.	For	other	parameters,	we	use	the	default	values	in	Detectron.	During	testing,	the	confidence	

OCR:	threshold	of	generating	bounding	boxes	is	set	to	90% 	null	For	table	structure	recognition,	we	use	the	open	source	framework	
GT :	threshold	of	generating	bounding	boxes	is	set	to	null	90%.	For	table	structure	recognition,	we	use	the	open	source	framework	

OCR:	OpenNMT	[Klein	et	al.,	2017]	to	train	the	image-	to-text	model.	OpenNMT	is	mainly	designed	for	neural	machine	translation,	which	
GT :	OpenNMT	[Klein	et	al.,	2017]	to	train	the	image-	to-text	model.	OpenNMT	is	mainly	designed	for	neural	machine	translation,	which	

OCR:	supports	many	encoder-decoder	frameworks.	In	this	task,	we	train	our	model	using	the	image-	to-text	method	in	OpenNMT.	The	model	
GT :	supports	many	encoder-decoder	frameworks.	In	this	task,	we	train	our	model	using	the	image-	to-text	method	in	OpenNMT.	The	model	

OCR:	is	also	trained	us-	ing	4xP100	null  	NVIDIA	GPUs	with	the	learning	rate	of	0.1	and	batch	size	of	24.	
GT :	is	also	trained	us-	ing	null  	4×P100	NVIDIA	GPUs	with	the	learning	rate	of	0.1	and	batch	size	of	24.	

OCR:	In	this	task,	the	vocabulary	size	of	the	output	space	is	small,	including	<tabular>,	</tabular>,	<thead>,	</thead>,	<tbody>,	</tbody>,	<tr>,	
GT :	In	this	task,	the	vocabulary	size	of	the	output	space	is	small,	including	<tabular>,	</tabular>,	<thead>,	</thead>,	<tbody>,	</tbody>,	<tr>,	

OCR:	</tr>,	<td>,	</td>,	<cell_y>,	<cell_n>.	null 	null	null 	null	For	other	parameters,	we	use	the	default	values	in	OpenNMT.	5.3	
GT :	</tr>,	<td>,	</td>,	null     	null     	<cell	y>, 	<cell	n>. 	For	other	parameters,	we	use	the	default	values	in	OpenNMT.	5.3	

OCR:	Results	The	evaluation	results	of	table	detection	models	are	shown	in	Table	2.	We	observe	that	models	perform	well	on	
GT :	Results	The	evaluation	results	of	table	detection	models	are	shown	in	Table	2.	We	observe	that	models	perform	well	on	

OCR:	the	same	domain.	For	instance,	the	ResNext-152	null       	model	trained	with	Word	documents	achieves	an	F1	score	of	0.9166	on	
GT :	the	same	domain.	For	instance,	the	null       	ResNeXt-152	model	trained	with	Word	documents	achieves	an	F1	score	of	0.9166	on	

OCR:	the	Word	dataset,	which	is	much	higher	than	the	F1	score	(0.8094)	on	Latex	documents.	Meanwhile,	the	ResNext-152	null       	model	
GT :	the	Word	dataset,	which	is	much	higher	than	the	F1	score	(0.8094)	on	Latex	documents.	Meanwhile,	the	null       	ResNeXt-152	model	

OCR:	trained	with	Latex	documents	achieves	an	F1	score	of	0.9810	on	the	Latex	dataset,	which	is	also	much	higher	than	
GT :	trained	with	Latex	documents	achieves	an	F1	score	of	0.9810	on	the	Latex	dataset,	which	is	also	much	higher	than	

OCR:	test	null 	ing	on	the	Word	documents	(0.8863).	This	indicates	that	the	tables	from	different	types	of	documents	have	different	
GT :	null	test-	ing	on	the	Word	documents	(0.8863).	This	indicates	that	the	tables	from	different	types	of	documents	have	different	

OCR:	vi-	sual	appearance.	Therefore,	we	cannot	simply	rely	on	trans-	fer	learning	techniques	to	obtain	good	table	detection	mod-	els	
GT :	vi-	sual	appearance.	Therefore,	we	cannot	simply	rely	on	trans-	fer	learning	techniques	to	obtain	good	table	detection	mod-	els	

OCR:	with	small	scale	training	data.	When	combining	training	data	with	Word	and	Latex	documents,	the	accuracy	of	larger	models	is	
GT :	with	small	scale	training	data.	When	combining	training	data	with	Word	and	Latex	documents,	the	accuracy	of	larger	models	is	

OCR:	comparable	to	models	trained	on	the	same	domain,	while	it	performs	better	on	the	Word+Latex	dataset.	This	ver-	ifies	that	
GT :	comparable	to	models	trained	on	the	same	domain,	while	it	performs	better	on	the	Word+Latex	dataset.	This	ver-	ifies	that	

OCR:	model	trained	with	larger	data	generalizes	better	on	different	domains,	which	illustrates	the	importance	of	creat-	ing	larger	benchmark	dataset.	
GT :	model	trained	with	larger	data	generalizes	better	on	different	domains,	which	illustrates	the	importance	of	creat-	ing	larger	benchmark	dataset.	

OCR:	In	addition,	we	also	evaluate	our	models	on	the	ICDAR	2013	table	competition	dataset.	Among	all	the	models,	the	Latex	
GT :	In	addition,	we	also	evaluate	our	models	on	the	ICDAR	2013	table	competition	dataset.	Among	all	the	models,	the	Latex	

OCR:	ResNext-152	null       	model	achieves	the	best	FI  	null	score	of	0.9625,	which	is	better	than	the	ResNext-152	null       	model	trained	
GT :	null       	ResNeXt-152	model	achieves	the	best	null	F1  	score	of	0.9625,	which	is	better	than	the	null       	ResNeXt-152	model	trained	

OCR:	on	Word+Latex	dataset	(0.9328).	This	shows	that	the	domain	of	the	ICDAR	2013	dataset	is	more	similar	to	the	Latex	
GT :	on	Word+Latex	dataset	(0.9328).	This	shows	that	the	domain	of	the	ICDAR	2013	dataset	is	more	similar	to	the	Latex	

OCR:	doc-	uments.	Furthermore,	we	evaluate	the	model	trained	with	the	DeepFigures	dataset	[Siegel	et	al.,	2018]	that	contains	more	han 	
GT :	doc-	uments.	Furthermore,	we	evaluate	the	model	trained	with	the	DeepFigures	dataset	[Siegel	et	al.,	2018]	that	contains	more	null	

OCR:	null	one	million	training	instances,	which	achieves	an	F1	score	of	0.8918.	This	also	indicates	that	more	training	data	does	
GT :	than	one	million	training	instances,	which	achieves	an	F1	score	of	0.8918.	This	also	indicates	that	more	training	data	does	

OCR:	not	always	lead	to	better	results	and	might	introduce	some	noise.	Therefore,	we	not	only	need	large	scale	train-	ing	
GT :	not	always	lead	to	better	results	and	might	introduce	some	noise.	Therefore,	we	not	only	need	large	scale	train-	ing	

OCR:	data	but	also	high	quality	data.	
GT :	data	but	also	high	quality	data.	

