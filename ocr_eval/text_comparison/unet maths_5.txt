OCR:	IEEE	Access	W.	Ohyama	et	al.:	Detecting	MEs	in	Scientific	Document	Images	Using	a	U-Net	Trained	on	a	Diverse	Dataset	
GT :	IEEE	Access	W.	Ohyama	et	al.:	Detecting	MEs	in	Scientific	Document	Images	Using	a	U-Net	Trained	on	a	Diverse	Dataset	

OCR:	i   	input	sub-block	FIGURE	4.	The	U-Net	architecture	in	the	proposed	method.	symbols	are	white.	To	prevent	the	elimination	of	
GT :	null	null 	null     	FIGURE	4.	The	U-Net	architecture	in	the	proposed	method.	symbols	are	white.	To	prevent	the	elimination	of	

OCR:	thin	com-	ponents	in	the	document	image,	white	regions	are	dilated	by	1	pixel	in	each	direction	using	the	mathematical	
GT :	thin	com-	ponents	in	the	document	image,	white	regions	are	dilated	by	1	pixel	in	each	direction	using	the	mathematical	

OCR:	morphology	operation	null      	Overlapped	square	sub-blocks	are	defined	to	cover	entire	image	regions	and	extracted	to	be	the	inputs	to	
GT :	morphology	null     	operation.	Overlapped	square	sub-blocks	are	defined	to	cover	entire	image	regions	and	extracted	to	be	the	inputs	to	

OCR:	the	image	conversion	stage.	The	edges	of	the	input	document	image	are	wrapped	by	the	opposite	side	of	the	image	
GT :	the	image	conversion	stage.	The	edges	of	the	input	document	image	are	wrapped	by	the	opposite	side	of	the	image	

OCR:	when	the	sub-block	is	over	the	image	edges.	The	sub-block	operation	is	used	initially	to	deal	with	the	memory	size	
GT :	when	the	sub-block	is	over	the	image	edges.	The	sub-block	operation	is	used	initially	to	deal	with	the	memory	size	

OCR:	limits	[7].	Also,	the	sub-	block	operation	plays	a	role	as	data	augmentation	without	image	deformation	operations.	Generally,	ideal	estimation	
GT :	limits	[7].	Also,	the	sub-	block	operation	plays	a	role	as	data	augmentation	without	image	deformation	operations.	Generally,	ideal	estimation	

OCR:	of	variations	of	data	is	crucial	for	designing	data	augmentation	protocols.	The	proposed	method	assumes	that	a	flat-bed	scan-	ner	
GT :	of	variations	of	data	is	crucial	for	designing	data	augmentation	protocols.	The	proposed	method	assumes	that	a	flat-bed	scan-	ner	

OCR:	captures	the	input	images	so	that	the	input	images	do	not	contain	significant	image	deformation.	Consequently,	data	augmentation	with	such	
GT :	captures	the	input	images	so	that	the	input	images	do	not	contain	significant	image	deformation.	Consequently,	data	augmentation	with	such	

OCR:	nonlinear	image	deformation	is	not	required	null     	The	width	and	height	of	the	sub-blocks	are	parameters	that	affect	the	performance	
GT :	nonlinear	image	deformation	is	not	null    	required.	The	width	and	height	of	the	sub-blocks	are	parameters	that	affect	the	performance	

OCR:	of	the	proposed	method.	The	actual	size	of	the	sub-block	images	in	our	implementation	was	determined	by	the	results	of	
GT :	of	the	proposed	method.	The	actual	size	of	the	sub-block	images	in	our	implementation	was	determined	by	the	results	of	

OCR:	a	preliminary	experiment	discussed	in	IV-C	B.	ME	DETECTION	USING	U-NET	ME	detection	in	the	proposed	method	can	be	considered	
GT :	a	preliminary	experiment	discussed	in	IV-C	B.	ME	DETECTION	USING	U-NET	ME	detection	in	the	proposed	method	can	be	considered	

OCR:	as	an	image	conversion	task.	Figure	5	shows	examples	of	input,	output	and	ground	truth	images	of	the	ME	detection	
GT :	as	an	image	conversion	task.	Figure	5	shows	examples	of	input,	output	and	ground	truth	images	of	the	ME	detection	

OCR:	process.	As	shown	in	the	figure,	the	ME	detection	process	is	required	to	eliminate	regions	from	MEs	and	extract	the	
GT :	process.	As	shown	in	the	figure,	the	ME	detection	process	is	required	to	eliminate	regions	from	MEs	and	extract	the	

OCR:	CCs	that	construct	MEs.	We	use	the	U-Net	architecture	proposed	by	Ronneberger	et	al.	[7],	motivated	by	the	promising	achieve-	
GT :	CCs	that	construct	MEs.	We	use	the	U-Net	architecture	proposed	by	Ronneberger	et	al.	[7],	motivated	by	the	promising	achieve-	

OCR:	ment	of	its	semantic	segmentation	of	biomedical	images.	U-Net	is	an	FCN	architecture	that	was	proposed	for	the	segmentation	of	
GT :	ment	of	its	semantic	segmentation	of	biomedical	images.	U-Net	is	an	FCN	architecture	that	was	proposed	for	the	segmentation	of	

OCR:	biomedical	images.	By	introducing	skip	connections	between	corresponding	layers	in	the	encoder	and	decoder,	it	successfully	preserves	the	high-frequency	components	
GT :	biomedical	images.	By	introducing	skip	connections	between	corresponding	layers	in	the	encoder	and	decoder,	it	successfully	preserves	the	high-frequency	components	

OCR:	in	the	converted	output	images.	Figure	4	shows	the	actual	U-Net	configuration	in	the	pro-	posed	method.	The	network	mainly	
GT :	in	the	converted	output	images.	Figure	4	shows	the	actual	U-Net	configuration	in	the	pro-	posed	method.	The	network	mainly	

OCR:	consists	of	two	stages,	i.e	encoding	and	decoding	stages.	In	encoding	stage,	the	typ-	ical	CNN	architecture	is	employed.	The	
GT :	consists	of	two	stages,	i.e	encoding	and	decoding	stages.	In	encoding	stage,	the	typ-	ical	CNN	architecture	is	employed.	The	

OCR:	encoding	stage	con-	sists	of	multiple	applications	of	a	3	x   	null	3	convolution	with	a	144034	i   	FIGURE	5.	
GT :	encoding	stage	con-	sists	of	multiple	applications	of	a	3	null	×   	3	convolution	with	a	null  	null	FIGURE	5.	

OCR:	Examples	of	input,	output	and	ground	truth	images	for	image	conversion	using	U-Net.	1x  	1	null	null	padding	followed	by	
GT :	Examples	of	input,	output	and	ground	truth	images	for	image	conversion	using	U-Net.	null	1	×   	1   	padding	followed	by	

OCR:	a	rectified	linear	unit	(ReLU)	activate	function	and	a	2	x   	null	2	max-pooling	operation	for	down	sampling.	The	number	
GT :	a	rectified	linear	unit	(ReLU)	activate	function	and	a	2	null	×   	2	max-pooling	operation	for	down	sampling.	The	number	

OCR:	of	feature	maps	is	doubled	at	each	two	downsampling	steps.	The	decoding	stages	consists	of	an	upsampling	of	the	feature	
GT :	of	feature	maps	is	doubled	at	each	two	downsampling	steps.	The	decoding	stages	consists	of	an	upsampling	of	the	feature	

OCR:	map	followed	by	a	2x2	up-	convolution.	While	the	concatenation	of	feature	maps	in	the	original	U-Net	requires	the	cropping	
GT :	map	followed	by	a	2x2	up-	convolution.	While	the	concatenation	of	feature	maps	in	the	original	U-Net	requires	the	cropping	

OCR:	operation	because	there	is	loss	of	border	pixels	in	every	convolution,	the	proposed	method	does	not	employ	cropping	because	the	
GT :	operation	because	there	is	loss	of	border	pixels	in	every	convolution,	the	proposed	method	does	not	employ	cropping	because	the	

OCR:	overlapped	sub-blocks	can	recover	the	loss	to	each	other.	The	final	layer	employs	a	1x1	convolution	to	map	each	M-component	
GT :	overlapped	sub-blocks	can	recover	the	loss	to	each	other.	The	final	layer	employs	a	1x1	convolution	to	map	each	null       	

OCR:	null	null      	feature	vector	to	a	binary	output	image	null  	As	shown	by	Figure	4,	the	proposed	method	assumes	that	
GT :	M   	-component	feature	vector	to	a	binary	output	null 	image.	As	shown	by	Figure	4,	the	proposed	method	assumes	that	

OCR:	the	size	of	an	input	sub-block	is	determined	by	2	x   	2N. 	null	null	null	null	null	The	number	of	
GT :	the	size	of	an	input	sub-block	is	determined	by	2	null	null	N   	×   	2   	N   	.   	The	number	of	

OCR:	layers	in	the	encoder	and	decoder	stages	is	corresponding	to	the	sub-block	size.	The	base	number	of	feature	maps	M	
GT :	layers	in	the	encoder	and	decoder	stages	is	corresponding	to	the	sub-block	size.	The	base	number	of	feature	maps	M	

OCR:	=	64	is	determined	from	the	original	U-Net	imprementation.	We	implemented	and	trained	U-Net	to	convert	the	input	sub-block	image	
GT :	=	64	is	determined	from	the	original	U-Net	imprementation.	We	implemented	and	trained	U-Net	to	convert	the	input	sub-block	image	

OCR:	to	an	image	that	contained	only	the	CCs	that	constructed	MEs.	To	achieve	this	conversion,	we	created	ground	truth	images	
GT :	to	an	image	that	contained	only	the	CCs	that	constructed	MEs.	To	achieve	this	conversion,	we	created	ground	truth	images	

OCR:	using	an	annotated	dataset.	In	the	dataset,	we	determined	whether	each	character	was	a	mathematical	symbol	or	ordinary	character.	We	
GT :	using	an	annotated	dataset.	In	the	dataset,	we	determined	whether	each	character	was	a	mathematical	symbol	or	ordinary	character.	We	

OCR:	eliminated	the	CC's	null	anno-	tated	as	ordinary	characters	to	create	the	ground	truth	images.	The	ground	truth	in	the	
GT :	eliminated	the	null	CCs 	anno-	tated	as	ordinary	characters	to	create	the	ground	truth	images.	The	ground	truth	in	the	

OCR:	training	dataset	was	a	set	of	sub-block	images	that	were	extracted	at	the	corresponding	position	on	the	input	image.	We	
GT :	training	dataset	was	a	set	of	sub-block	images	that	were	extracted	at	the	corresponding	position	on	the	input	image.	We	

OCR:	used	the	Dice	loss	determined	by	the	fol-	lowing	as	the	objective	loss	function	to	be	minimized	because	the	task	
GT :	used	the	Dice	loss	determined	by	the	fol-	lowing	as	the	objective	loss	function	to	be	minimized	because	the	task	

OCR:	of	U-Net	is	binary-to-binary	image	conversion:	(1) 	L   	(X, 	Y)  	=   	1   	-   	D(X,	Y)  	=   	1   	-   	2   	x   	
GT :	of	U-Net	is	binary-to-binary	image	conversion:	null	null	null	null	null	null	null	null	null	null	null	null	null	null	

OCR:	nyl 	VOLUME	7,	2019	
GT :	null	VOLUME	7,	2019	

